TrainingArguments:
  num_train_epochs: 3
  per_device_train_batch_size: 8
  per_device_eval_batch_size: 8
  weight_decay: 0.01
  logging_steps: 200
  eval_strategy: steps
  save_strategy: steps
  learning_rate: 0.0002
  seed: 42

LoRAConfig:
  lora_r: 16
  lora_alpha: 32
  lora_target_modules: ["q_proj", "v_proj"]
  lora_dropout: 0.1